{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75940e88-9a83-4eda-be93-94d82fc3d44a",
   "metadata": {},
   "source": [
    "## Introduction Machine Learning Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7feb0371-6d90-4bbb-b50e-389b8af5d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "\n",
    "# Overfitting: Overfitting in ML is when a training model performs with high accuracy where the test model performs very low. \n",
    "# Underfitting: Underfitting in ML is when a training model performs with low accuracy where the test model performs low than \n",
    "# the training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b64f34f-ccbd-40dd-ae5b-c72cf41a72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "\n",
    "# Cross-validation: Split the dataset into multiple subsets for traing and validation. This helps assess the model's \n",
    "# performance on different data points.\n",
    "\n",
    "#Feature selection: By choosing only the most relevant features or use techniques like PCA to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc3b560-5d55-4388-9606-0c2c2c5326bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "\n",
    "# Scenarios where underfitting can occur in machine learning include:\n",
    "\n",
    "# Simple Models: Using models that lack complexity or have too few parameters to capture the underlying patterns in the data. \n",
    "# For instance, a linear model applied to a highly non-linear dataset may result in underfitting.\n",
    "\n",
    "# Insufficient Training: When the model is not trained for enough epochs or iterations, especially in complex models like \n",
    "# neural networks, it might not learn the intricate relationships present in the data.\n",
    "\n",
    "# Small Dataset: Training a complex model on a small dataset can lead to underfitting. The model might not generalize well \n",
    "# because it hasn't seen enough examples to learn the underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b43e56-0612-4f5b-8e14-ad25698ad7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "\n",
    "# The bias-variance tradeoff is a fundamental concept in machine learning that illustrates the relationship between a model's \n",
    "# ability to learn from data and its performance on unseen data.\n",
    "\n",
    "# The relationship between bias and variance can be understood in the context of model performance:\n",
    "\n",
    "# High Bias, Low Variance (Underfitting): When a model has high bias and low variance, it means the model is too simple and \n",
    "# doesn't fit the training data well. It fails to capture the underlying patterns and performs poorly on both the training \n",
    "# and test data.\n",
    "\n",
    "# Low Bias, High Variance (Overfitting): Conversely, when a model has low bias and high variance, it means the model is highly\n",
    "# complex and fits the training data very well. However, it captures noise and random fluctuations, leading to poor performance\n",
    "# on new, unseen data.\n",
    "\n",
    "# Optimal Model: Ideally, you want a model that has both low bias and low variance, effectively capturing the underlying \n",
    "# patterns in the data without overfitting to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad24aadc-33c0-4d0e-938f-4ca25d8b5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "\n",
    "# Determining Overfitting or Underfitting:\n",
    "\n",
    "# Validation Set Performance: Compare the model's performance on a separate validation set. If the model performs well on the\n",
    "# training set but poorly on the validation set, it might be overfitting. If it performs poorly on both, it might be \n",
    "# underfitting.\n",
    "\n",
    "# Model Complexity vs. Performance: Adjust the model complexity (e.g., number of parameters, features, or layers) and observe\n",
    "# changes in performance. If increasing complexity improves validation performance initially but worsens it later, it might\n",
    "# indicate overfitting. If increasing complexity doesn't improve performance, it might be underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e400d6d-af32-4e1f-87c3-c9629a991610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6\n",
    "\n",
    "# Comparision\n",
    "\n",
    "# Bias:\n",
    "# High bias models are overly simplistic and fail to capture the underlying patterns in the data. They lead to underfitting \n",
    "# and perform poorly both on training and test data.\n",
    "\n",
    "# These models have limited capacity to learn complex patterns, making them too rigid to capture the nuances of the data.\n",
    "\n",
    "# Examples:\n",
    "# A linear regression model applied to a complex non-linear dataset might exhibit high bias. It's too simple to capture the \n",
    "# intricate relationships present in the data, resulting in underfitting.\n",
    "\n",
    "# Performs poorly on both the training and test/validation datasets due to oversimplification, resulting in consistently \n",
    "# high errors.\n",
    "\n",
    "# Variance:\n",
    "# High variance models are overly complex and capture noise or random fluctuations in the training data. They lead to \n",
    "# overfitting and perform well on the training data but poorly on new, unseen data.\n",
    "\n",
    "# These models are highly flexible and may capture noise, outliers, or specific patterns in the training data that \n",
    "# don't generalize well.\n",
    "\n",
    "# Examples:\n",
    "# A decision tree with no depth limit or a deep neural network trained on a small dataset might display high variance. \n",
    "# These models can fit the training data very well but fail to generalize to new data due to overfitting.\n",
    "\n",
    "# Performs very well on the training dataset but poorly on the test/validation dataset due to overfitting. It shows a large\n",
    "# gap between the performance on training and test data, indicating that it fails to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6ffd6-8633-4cee-a021-67b63fb836cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "\n",
    "# Regularization in machine learning refers to techniques used to prevent overfitting by adding constraints or penalties to the\n",
    "# model's learning process. These constraints discourage the model from becoming overly complex or fitting noise in the \n",
    "# training data, thereby improving its ability to generalize to new, unseen data.\n",
    "\n",
    "# Common Regularization Techniques:\n",
    "\n",
    "# L1 Regularization (Lasso): Adds a penalty term proportional to the absolute value of the coefficients' sum to the loss \n",
    "# function.\n",
    "\n",
    "# L2 Regularization (Ridge): Adds a penalty term proportional to the square of the coefficients' sum to the loss function.\n",
    "\n",
    "# Dropout (Neural Networks): During training, randomly deactivates (sets to zero) a proportion of neurons in each layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
